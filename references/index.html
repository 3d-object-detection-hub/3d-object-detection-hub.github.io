<!DOCTYPE html>
<html lang="en">
<!-- _includes/head.html -->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>References | 3D Object Detection Hub</title>
  <meta name="description" content="A comprehensive resource aggregating benchmarks, methods and results from my master thesis and the broader 3D object detection community.">

  
  

  <!-- favicon -->
  <link rel="icon" href="/favicon.ico" type="image/x-icon">

  <!-- custom dark-mode & layout overrides -->
  <link rel="stylesheet" href="/assets/styles.css">

  <!-- DataTables CSS -->
  <link
    rel="stylesheet"
    href="https://cdn.datatables.net/1.13.6/css/jquery.dataTables.min.css" />

  <!-- your overrides -->
  <link
    rel="stylesheet"
    href="/assets/datatables-overrides.css" />

  <!-- PapaParse, jQuery & DataTables JS -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/PapaParse/5.4.1/papaparse.min.js"></script>
  <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
  <script
    src="https://cdn.datatables.net/1.13.6/js/jquery.dataTables.min.js"></script>
    
</head>

<body>
  <header class="site-header">
    <div class="site-content">
      <h1 class="site-title">
        <a href="/">3D Object Detection Hub</a>
      </h1>
      <nav>
        <ul>
          <li><a href="/">Home</a></li>
          <li><a href="/datasets/">Datasets</a></li>
          <li><a href="/3d-object-detection/">Models</a></li>
          <li><a href="/references/">References</a></li>
          <li><a href="/about/">About</a></li>
        </ul>
      </nav>
    </div>
  </header>

  <main class="site-content">
    <section class="site-content">

</section>

<h1 id="references">References</h1>

<ol class="bibliography"><li><span id="Geddes:1940">Gueddes, N. B. (1940). <i>Magic Motorways</i>. Random House.</span></li>
<li><span id="racecar">Brown, M. (2011). <i>Racecar - Searching for the Limit in Formula SAE</i>. Seven Car.</span></li>
<li><span id="Morgado:MSc">Morgado, D. (2021). <i>A Perception Pipeline for an Autonomous Formula Student Vehicle</i> [MSc Thesis in Mechanical Engineering]. Universidade de Lisboa - Instituto Superior Técnico.</span></li>
<li><span id="gomes2024learning">Gomes, D. R., Botto, M. A., &amp; Lima, P. U. (2024). Learning-based Model Predictive Control for an Autonomous Formula Student Racing Car [Master's thesis, Universidade de Lisboa - Instituto Superior Técnico]. In <i>2024 IEEE International Conference on Robotics and Automation (ICRA)</i> (pp. 12556–12562). https://ieeexplore.ieee.org/abstract/document/10611285</span></li>
<li><span id="Jose:2016">Jose, C. P. (2016). A review on the trends and developments in hybrid electric vehicle. <i>Innovative Design and Development Practices in Aerospace and Automotive Engineering: I-DAD</i>, 211–229. https://link.springer.com/chapter/10.1007/978-981-10-1771-1_25</span></li>
<li><span id="Stanchev">Stanchev, P., &amp; Geske, J. (2016). Autonomous Cars. History. State of Art. Research Problems. <i>DCCN 2015</i>, 1–10. https://link.springer.com/chapter/10.1007/978-3-319-30843-2_1</span></li>
<li><span id="Aggarwal:2022">Aggarwal, I. (2022). Rise of Autonomous Vehicles. <i>International Journal of Social Science and Economic Research</i>, <i>7</i>(10). https://ijsser.org/2022files/ijsser_07__229.pdf</span></li>
<li><span id="Brummelen:2018">Brummelen, J. V., O’Brien, M., Gruyerb, D., &amp; Najjaran, H. (2018). Autonomous vehicle perception: The technology of today and tomorrow. <i>Transportation Research: Part C</i>, 89: 384–406. https://www.sciencedirect.com/science/article/pii/S0968090X18302134</span></li>
<li><span id="SAE:norm">of Automotive Engineers, S. (2022). Taxonomy and Definitions for Terms Related to Driving Automation Systems for On-Road Motor Vehicles. <i>J3016_202104</i>. https://www.sae.org/standards/content/j3016_202104/</span></li>
<li><span id="Betz:2022">Betz, J., Zheng, H., Liniger, A., Rosolia, U., Karle, P., Behl, M., Krovi, V., &amp; Mangharam, R. (2022). Autonomous Vehicles on the Edge: A Survey on Autonomous Vehicle Racing. <i>IEEE Open Journal of Intelligent Transportation Systems</i>, 3: 458–488. https://ieeexplore.ieee.org/abstract/document/9790832</span></li>
<li><span id="Fayyad:2020">Fayyad, J., Jaradat, M. A., Gruyer, D., &amp; Najjaranngharam, H. (2022). Deep Learning Sensor Fusion: Vehicle Perception and Localization: A Review. <i>Sensors</i>, 20. https://www.mdpi.com/1424-8220/20/15/4220</span></li>
<li><span id="IDTechEx:2023">Jeffs, J., &amp; He, M. X. (2023). Autonomous Cars, Robotaxis and Sensors 2024-2044. <i>IDTechEx</i>. https://www.idtechex.com/en/research-report/autonomous-cars-robotaxis-and-sensors-2024-2044/953
   </span></li>
<li><span id="Ieee">Ackerman, E. (2021). What Full Autonomy Means for the Waymo Driver. <i>IEE Spectrum</i>. https://spectrum.ieee.org/full-autonomy-waymo-driver</span></li>
<li><span id="Betz:2019">Betz, J., &amp; al., E. (2019). What can we learn from autonomous level-5 motorsport? <i>Springer</i>. https://link.springer.com/content/pdf/10.1007/978-3-658-22050-1_12.pdf
   </span></li>
<li><span id="Barrachina:2013">Barrachina, J., &amp; al., E. (2013). V2X-d: A vehicular density estimation system that combines V2V and V2I communications. <i>IFIP Wireless Days (WD)</i>. https://ieeexplore.ieee.org/document/6686518</span></li>
<li><span id="Dall">Dhall, A., Dai, D., &amp; Gool, L. V. (2019). Real-time 3D Traffic Cone Detection for Autonomous Driving. <i>IEEE Intelligent Vehicles Symposium</i>. https://ieeexplore.ieee.org/document/8814089/
   </span></li>
<li><span id="Wen">Wen, L., &amp; Jo, K. (2022). Deep learning-based perception systems for autonomous driving: A comprehensive survey. <i>Neuralcomputing</i>. https://doi.org/10.1016/j.neucom.2021.08.155</span></li>
<li><span id="pcl">Rusu, R. B., &amp; Cousins, S. (2011). 3d is here: Point Cloud Library (PCL). <i>IEEE International Conference on Robotics And
Automation</i>. https://ieeexplore.ieee.org/document/5980567</span></li>
<li><span id="nyungen">Nguyen, A., &amp; Jo, K. (2013). 3D Point Cloud Segmentation: A survey. <i>IEEE Conference on Robotics, Automation and Mechatronics</i>. https://ieeexplore.ieee.org/document/6758588</span></li>
<li><span id="liu">Liu, W., Sun, J., Li, W., Hu, T., &amp; Wang, P. (2019). Deep Learning on Point Clouds and Its Application:
A Survey. <i>Sensors</i>. https://www.mdpi.com/1424-8220/19/19/4188</span></li>
<li><span id="Zhang">Zhang, J., Zhao, X., &amp; Lu, Z. (2019). A Review of Deep Learning-Based Semantic
Segmentation for Point Cloud. <i>IEEE Access</i>. https://ieeexplore.ieee.org/abstract/document/8930503/</span></li>
<li><span id="Grilli">Grilli, E., Menna, F., &amp; Remondino, F. (2017). A Review of Point Clouds Segmentation and Classification Algorithms. <i>International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences</i>. https://isprs-archives.copernicus.org/articles/XLII-2-W3/339/2017/</span></li>
<li><span id="Bhanu">Bhanu, B., Lee, S., Ho, C., &amp; Henderson, T. (1986). Range data processing:
Representation of surfaces by edges. <i>Pattern Recognition</i>. https://core.ac.uk/download/pdf/276277383.pdf</span></li>
<li><span id="Sappa">Sappa, A., &amp; M.Devy. (2001). Fast range image segmentation by an edge detection strategy. <i>3D Digital Imaging and Modeling</i>. https://ieeexplore.ieee.org/abstract/document/924460</span></li>
<li><span id="Jiang">Jiang, X. Y., Bunke, H., &amp; Meier, U. (1996). Fast range image segmentation. <i>Third IEEE</i>. https://ieeexplore.ieee.org/document/572006/</span></li>
<li><span id="Bello">Bello, S. A., &amp; al, E. (2020). Review: Deep Learning on 3D Point Clouds. <i>Remote Sensing</i>. https://www.mdpi.com/2072-4292/12/11/1729</span></li>
<li><span id="Cheri">Cheri, A., &amp; Mouftah, H. T. (2019). Autonomous vehicles in the sustainable cities, the beginning of a green adventure. <i>Sustainable Cities and Society</i>. https://doi.org/10.1016/j.scs.2019.101751</span></li>
<li><span id="Sense">Srivastava, A. (2019). Sense-Plan-Act in Robotic Applications. <i>DOI, 10: 3</i>.</span></li>
<li><span id="Dingus">Dingus, T. D., &amp; al, E. (2016). Driver crash risk factors and prevalence evaluation using driving data. <i>Proceedings of the National Academy of Sciences</i>. https://www.pnas.org/doi/abs/10.1073/pnas.1513271113
   </span></li>
<li><span id="gottard">Gosala, N., &amp; al., E. (2019). Redundant Perception and State Estimation for Reliable Autonomous Racing. <i>2019 International Conference on Robotics and Automation (ICRA)</i>. https://ieeexplore.ieee.org/document/8794155</span></li>
<li><span id="fluela">Valls, M., &amp; al., E. (2018). Design of an Autonomous Racecar: Perception, State Estimation and System Integration. <i>2018 IEEE ICRA</i>. https://ieeexplore.ieee.org/document/8462829</span></li>
<li><span id="people">Hudson, J., Orviska, M., &amp; Hunady, J. (2019). People’s attitudes to autonomous vehicles. <i>Transportation Research Part A: Policy and Practice</i>, 121: 164–176. https://doi.org/10.1016/j.tra.2018.08.018</span></li>
<li><span id="betz2023tum">Betz, J., &amp; al., E. (2023). Tum autonomous motorsport: An autonomous racing software for the indy autonomous challenge. <i>Journal of Field Robotics</i>, <i>40</i>(4), 783–809. https://doi.org/10.1002/rob.22153</span></li>
<li><span id="fsoco">Vödisch, N., Dodel, D., &amp; Schötz, M. (2022). FSOCO: The Formula Student Objects in Context Dataset. <i>SAE International Journal of Connected and Automated Vehicles</i>, <i>5</i>. https://arxiv.org/abs/2012.07139</span></li>
<li><span id="waymosafe">LLC, W. (2021). <i>On the road to fully self-driving. Waymo Safety Report</i> (pp. 1–48).</span></li>
<li><span id="osti_10221872">O’Kelly, M., Zheng, H., Karthik, D., &amp; Mangharam, R. (2020). F1TENTH: An Open-source Evaluation Environment for Continuous Control and Reinforcement Learning. <i>Proceedings of ML Research</i>, <i>123</i>. https://par.nsf.gov/biblio/10221872</span></li>
<li><span id="yurtsever2020survey">Yurtsever, E., Lambert, J., Carballo, A., &amp; Takeda, K. (2020). A survey of autonomous driving: Common practices and emerging technologies. <i>IEEE Access</i>, <i>8</i>, 58443–58469. https://ieeexplore.ieee.org/abstract/document/9046805</span></li>
<li><span id="HULSE20181">Hulse, L. M., Xie, H., &amp; Galea, E. R. (2018). Relationships with road users, risk, gender and age. <i>Safety Science</i>, <i>102</i>, 1–13. https://www.sciencedirect.com/science/article/pii/S0925753517306999</span></li>
<li><span id="montgomery2018america">Montgomery, W., &amp; al., E. (2018). Realizing productivity gains and spurring economic growth. <i>America’s Workforce and the Self-Driving Future</i>. https://avworkforce.secureenergy.org/</span></li>
<li><span id="singh2015critical">Singh, S. (2015). Critical reasons for crashes investigated in the national motor vehicle crash causation survey. <i>National Highway Traffic Safety Administration</i>. http://www-nrd.nhtsa.dot.gov/Pubs/812115.pdf</span></li>
<li><span id="Formula">FSG. (2023). <i>FS Rules 2024</i>. https://www.formulastudent.de/fsg/rules/</span></li>
<li><span id="hand">FSG. (2023). <i>FS Handbook 2024</i>. https://www.formulastudent.de/fsg/rules/</span></li>
<li><span id="arnold2019survey">Arnold, E., Al-Jarrah, O. Y., &amp; al., E. (2019). A survey on 3d object detection methods for autonomous driving applications. <i>IEEE Transactions on Intelligent Transportation Systems</i>, <i>20</i>(10), 3782–3795. https://ieeexplore.ieee.org/abstract/document/8621614</span></li>
<li><span id="geiger2013vision">Geiger, A., Lenz, P., Stiller, C., &amp; Urtasun, R. (2013). Vision meets robotics: The kitti dataset. <i>The International Journal of Robotics Research</i>, <i>32</i>(11), 1231–1237. https://journals.sagepub.com/doi/full/10.1177/0278364913491297</span></li>
<li><span id="liang2021survey">Liang, W., Xu, P., Guo, L., Bai, H., Zhou, Y., &amp; Chen, F. (2021). A survey of 3D object detection. <i>Multimedia Tools and Applications</i>, <i>80</i>(19), 29617–29641. https://link.springer.com/article/10.1007/s11042-021-11137-y</span></li>
<li><span id="qian2023d">Qian, R., Lai, X., &amp; Li, X. (2022). 3D object detection for autonomous driving: A survey. <i>Pattern Recognition</i>, <i>130</i>, 108796. https://www.sciencedirect.com/science/article/pii/S0031320322002771</span></li>
<li><span id="mao20233d">Mao, J., Shi, S., Wang, X., &amp; Li, H. (2023). 3D object detection for autonomous driving: A comprehensive survey. <i>International Journal of Computer Vision</i>, <i>131</i>(8), 1909–1963. https://link.springer.com/article/10.1007/s11263-023-01790-1</span></li>
<li><span id="wang2023multi">Wang, Y., Mao, Q., &amp; al., E. (2023). Multi-modal 3d object detection in autonomous driving: a survey. <i>International Journal of Computer Vision</i>, <i>131</i>(8), 2122–2152. https://link.springer.com/article/10.1007/s11263-023-01784-z</span></li>
<li><span id="nagiufab20243d">Nagiub, A. S., Fayez, M., Khaled, H., &amp; Ghoniemy, S. (2024). 3D object detection for autonomous driving: a comprehensive review. <i>2024 6th International Conference on Computing and Informatics (ICCI)</i>, 01–11. https://ieeexplore.ieee.org/abstract/document/10485120</span></li>
<li><span id="calvo2023timepillars">Calvo, E. L., Taveira, B., Kahl, F., Gustafsson, N., Larsson, J., &amp; Tonderski, A. (2023). Timepillars: Temporally-recurrent 3d lidar object detection. <i>ArXiv Preprint ArXiv:2312.17260</i>. https://arxiv.org/abs/2312.17260</span></li>
<li><span id="xuan2024multimodal">Xuan, Y., &amp; Qu, Y. (2024). Multimodal Data Fusion for BEV Perception. <i>Master Thesis</i>. https://odr.chalmers.se/items/589548c4-f439-4c12-ac16-6d74884ec41b</span></li>
<li><span id="vaswani2017attention">Vaswani, A., &amp; al., E. (2017). Attention is all you need. <i>Advances in Neural Information Processing Systems</i>, <i>30</i>. https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html</span></li>
<li><span id="dosovitskiy2020image">Dosovitskiy, A., &amp; al., E. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. <i>ArXiv Preprint ArXiv:2010.11929</i>. https://arxiv.org/pdf/2010.11929/1000</span></li>
<li><span id="chi2020relationnet++">Chi, C., Wei, F., &amp; Hu, H. (2020). Relationnet++: Bridging visual representations for object detection via transformer decoder. <i>Advances in Neural Information Processing Systems</i>, <i>33</i>, 13564–13574. https://arxiv.org/abs/2010.15831</span></li>
<li><span id="gao2025deep">Gao, W., &amp; Li, G. (2025). <i>Deep learning for 3D point clouds</i>. Springer. https://link.springer.com/content/pdf/10.1007/978-981-97-9570-3.pdf</span></li>
<li><span id="alhardiobject">Alhardi, A., &amp; Afeef, M. A. (2024). Object Detection Algorithms &amp; Techniques. <i>4th International Conference on Innovative Academic Studies</i>.</span></li>
<li><span id="goodfellow2016deep">Goodfellow, I., Bengio, Y., Courville, A., &amp; Bengio, Y. (2016). <i>Deep learning</i>. MIT press Cambridge.</span></li>
<li><span id="krizhevsky2017imagenet">Krizhevsky, A., Sutskever, I., &amp; Hinton, G. E. (2017). ImageNet classification with deep convolutional neural networks. <i>Communications of the ACM</i>, <i>60</i>(6), 84–90. https://dl.acm.org/doi/abs/10.1145/3065386</span></li>
<li><span id="cv">Moutinho, A. (2022). <i>Computer Vision Slides</i>. Instituto Superior Técnico.</span></li>
<li><span id="li2020review">Li, K., &amp; Cao, L. (2020). A review of object detection techniques. <i>2020 5th International Conference on ICECTT</i>, 385–390. https://ieeexplore.ieee.org/abstract/document/9237557</span></li>
<li><span id="ng2016artificial">Ng, A. (2016). What artificial intelligence can and can’t do right now. <i>Harvard Business Review</i>, <i>9</i>(11).</span></li>
<li><span id="turk1994zippered">Turk, G., &amp; Levoy, M. (1994). Zippered polygon meshes from range images. <i>Proceedings of the 21st Annual Conference on Computer Graphics and Interactive Techniques</i>. https://dl.acm.org/doi/abs/10.1145/192161.192241</span></li>
<li><span id="lin2025high">Lin, H., Wang, L., Qu, X., &amp; others. (2025). A High-Precision Calibration and Evaluation Method Based on Binocular Cameras and LiDAR for Intelligent Vehicles. <i>IEEE Transactions on Vehicular Technology</i>.</span></li>
<li><span id="zhang20253d">Zhang, H., &amp; al., E. (2025). 3D LiDAR and monocular camera calibration: A Review. <i>IEEE Sensors Journal</i>. https://ieeexplore.ieee.org/abstract/document/10852582</span></li>
<li><span id="huch2025lidar">Huch, H. C. S. (2025). <i>LiDAR Domain Adaptation for Perception of Autonomous Vehicles</i> [PhD thesis, Technische Universität München]. https://mediatum.ub.tum.de/1748697</span></li>
<li><span id="garcia2025fine">Garcia, G. M., &amp; al., E. (2025). Fine-tuning image-conditional diffusion models is easier than you think. <i>2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</i>, 753–762. https://arxiv.org/abs/2409.11355</span></li>
<li><span id="yang2024depth">Yang, L., Kang, B., &amp; al., E. (2024). Depth anything: Unleashing the power of large-scale unlabeled data. <i>Proceedings of the IEEE/CVF Conference on CVPR</i>, 10371–10381. https://arxiv.org/abs/2401.10891</span></li>
<li><span id="yang2024depth2">Yang, L., Kang, B., &amp; al., E. (2024). Depth anything v2. <i>Advances in Neural Information Processing Systems</i>, <i>37</i>, 21875–21911. https://arxiv.org/abs/2406.09414</span></li>
<li><span id="peris2012towards">Peris, M., Martull, S., Maki, A., Ohkawa, Y., &amp; Fukui, K. (2012). Towards a simulation driven stereo vision system. <i>Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012)</i>, 1038–1042. https://ieeexplore.ieee.org/abstract/document/6460313</span></li>
<li><span id="lidelving">Li, H., Zhao, Y., Zhong, J., Wang, B., Sun, C., &amp; Sun, F. (2025). Delving into the Secrets of BEV 3D Object Detection in Autonomous Driving: A Comprehensive Survey. <i>Authorea Preprints</i>. https://www.techrxiv.org/doi/full/10.36227/techrxiv.173221675.59410416</span></li>
<li><span id="caesar2020nuscenes">Caesar, H., Bankiti, V., &amp; al., E. (2020). nuscenes: A multimodal dataset for autonomous driving. <i>Proceedings of the IEEE/CVF Conference on CVPR</i>, 11621–11631. https://arxiv.org/abs/1903.11027</span></li>
<li><span id="waymo">Sun, P., Kretzschmar, H., Dotiwalla, X., Chouard, A., &amp; others. (2020). Scalability in perception for autonomous driving: Waymo open dataset. <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</i>, 2446–2454. https://arxiv.org/abs/1912.04838</span></li>
<li><span id="zamanakos2021comprehensive">Zamanakos, G., Tsochatzidis, L., Amanatiadis, A., &amp; Pratikakis, I. (2021). A comprehensive survey of LIDAR-based 3D object detection methods with deep learning for autonomous driving. <i>Computers &amp; Graphics</i>, <i>99</i>, 153–181. https://www.sciencedirect.com/science/article/pii/S0097849321001321</span></li>
<li><span id="cybenko1989approximation">Cybenko, G. (1989). Approximation by superpositions of a sigmoidal function. <i>Mathematics of Control, Signals and Systems</i>.</span></li>
<li><span id="block1962perceptron">Block, H.-D. (1962). The perceptron: A model for brain functioning. i. <i>Reviews of Modern Physics</i>, <i>34</i>(1), 123.</span></li>
<li><span id="qi2017pointnet">Qi, C. R., Su, H., Mo, K., &amp; Guibas, L. J. (2017). Pointnet: Deep learning on point sets for 3d classification and segmentation. <i>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</i>, 652–660. https://arxiv.org/abs/1612.00593</span></li>
<li><span id="qi2017pointnet++">Qi, C. R., Yi, L., Su, H., &amp; Guibas, L. J. (2017). Pointnet++: Deep hierarchical feature learning on point sets in a metric space. <i>Advances in Neural Information Processing Systems</i>, <i>30</i>. https://arxiv.org/abs/1706.02413</span></li>
<li><span id="lai2024survey">Lai-Dang, Q.-V. (2024). A survey of vision transformers in autonomous driving: Current trends and future directions. <i>ArXiv Preprint ArXiv:2403.07542</i>. https://arxiv.org/abs/2403.07542</span></li>
<li><span id="chang2019argoverse">Chang, M.-F., &amp; al., E. (2019). Argoverse: 3d tracking and forecasting with rich maps. <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</i>, 8748–8757. https://arxiv.org/abs/1911.02620</span></li>
<li><span id="patil2019h3d">Patil, A., Malla, S., Gang, H., &amp; Chen, Y.-T. (2019). The h3d dataset for full-surround 3d multi-object detection and tracking in crowded urban scenes. <i>2019 International Conference on Robotics and Automation (ICRA)</i>, 9552–9557. https://arxiv.org/abs/1903.01568</span></li>
<li><span id="houston2021one">Houston, J., &amp; al., E. (2021). One thousand and one hours: Self-driving motion prediction dataset. <i>Conference on Robot Learning</i>, 409–418. https://proceedings.mlr.press/v155/houston21a.html</span></li>
<li><span id="wang2019apolloscape">Wang, P., &amp; al., E. (2019). The apolloscape open dataset for autonomous driving and its application. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>, <i>1</i>.</span></li>
<li><span id="kuznetsova2020open">Kuznetsova, A., Rom, H., Alldrin, N., &amp; others. (2020). The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. <i>International Journal of Computer Vision</i>, <i>128</i>(7), 1956–1981. https://arxiv.org/abs/1811.00982z</span></li>
<li><span id="russakovsky2015imagenet">Russakovsky, O., Deng, J., Su, H., &amp; and others. (2015). Imagenet large scale visual recognition challenge. <i>International Journal of Computer Vision</i>, <i>115</i>, 211–252. https://arxiv.org/abs/1409.0575</span></li>
<li><span id="everingham2010pascal">Everingham, M., Van Gool, L., Williams, C. K. I., Winn, J., &amp; Zisserman, A. (2010). The pascal visual object classes (voc) challenge. <i>International Journal of Computer Vision</i>, <i>88</i>, 303–338. https://link.springer.com/article/10.1007/S11263-009-0275-4</span></li>
<li><span id="lin2014microsoft">Lin, T.-Y., &amp; al., E. (2014). Microsoft coco: Common objects in context. <i>Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part v 13</i>, 740–755. https://arxiv.org/abs/1405.0312</span></li>
<li><span id="silberman2012indoor">Silberman, N., Hoiem, D., Kohli, P., &amp; Fergus, R. (2012). Indoor segmentation and support inference from rgbd images. <i>Computer Vision–ECCV 2012: 12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part V 12</i>, 746–760. https://link.springer.com/chapter/10.1007/978-3-642-33715-4_54</span></li>
<li><span id="pravallika2024deep">Pravallika, A., Hashmi, M. F., &amp; Gupta, A. (2024). Deep Learning Frontiers in 3D Object Detection: A Comprehensive Review for Autonomous Driving. <i>IEEE Access</i>. https://ieeexplore.ieee.org/abstract/document/10670385/</span></li>
<li><span id="zhu2024systematic">Zhu, M., Gong, Y., Tian, C., &amp; Zhu, Z. (2024). A Systematic Survey of Transformer-Based 3D Object Detection for Autonomous Driving: Methods, Challenges and Trends. <i>Drones</i>, <i>8</i>(8), 412. https://www.mdpi.com/2504-446X/8/8/412</span></li>
<li><span id="bhat2021adabins">Bhat, S. F., Alhashim, I., &amp; Wonka, P. (2021). Adabins: Depth estimation using adaptive bins. <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</i>, 4009–4018. https://openaccess.thecvf.com/content/CVPR2021/html/Bhat_AdaBins_Depth_Estimation_Using_Adaptive_Bins_CVPR_2021_paper.html</span></li>
<li><span id="he2025distill">He, X., &amp; al., E. (2025). Distill Any Depth: Distillation Creates a Stronger Monocular Depth Estimator. <i>ArXiv Preprint ArXiv:2502.19204</i>. https://arxiv.org/abs/2502.19204</span></li>
<li><span id="fu2018deep">Fu, H., &amp; al., E. (2018). Deep ordinal regression network for monocular depth estimation. <i>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</i>, 2002–2011. https://openaccess.thecvf.com/content_cvpr_2018/html/Fu_Deep_Ordinal_Regression_CVPR_2018_paper.html</span></li>
<li><span id="ranftl2020towards">Ranftl, R., &amp; al., E. (2020). Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>, <i>44</i>(3), 1623–1637. https://ieeexplore.ieee.org/abstract/document/9178977</span></li>
<li><span id="ranftl2021vision">Ranftl, R., Bochkovskiy, A., &amp; Koltun, V. (2021). Vision transformers for dense prediction. <i>Proceedings of the IEEE/CVF International Conference on Computer Vision</i>, 12179–12188. https://openaccess.thecvf.com/content/ICCV2021/html/Ranftl_Vision_Transformers_for_Dense_Prediction_ICCV_2021_paper.html</span></li>
<li><span id="li2023depthformer">Li, Z., Chen, Z., Liu, X., &amp; Jiang, J. (2023). Depthformer: Exploiting long-range correlation and local information for accurate monocular depth estimation. <i>Machine Intelligence Research</i>, <i>20</i>(6), 837–854. https://link.springer.com/article/10.1007/s11633-023-1458-0]
</span></li>
<li><span id="yin2023metric3d">Yin, W., &amp; al., E. (2023). Metric3d: Towards zero-shot metric 3d prediction from a single image. <i>Proceedings of the IEEE/CVF International Conference on Computer Vision</i>, 9043–9053. https://openaccess.thecvf.com/content/ICCV2023/html/Yin_Metric3D_Towards_Zero-shot_Metric_3D_Prediction_from_A_Single_Image_ICCV_2023_paper.html</span></li>
<li><span id="bhat2023zoedepth">Bhat, S. F., &amp; al., E. (2023). Zoedepth: Zero-shot transfer by combining relative and metric depth. <i>ArXiv Preprint ArXiv:2302.12288</i>.</span></li>
<li><span id="ke2024repurposing">Ke, B., Obukhov, A., &amp; al., E. (2024). Repurposing diffusion-based image generators for monocular depth estimation. <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</i>, 9492–9502. https://openaccess.thecvf.com/content/CVPR2024/html/Ke_Repurposing_Diffusion-Based_Image_Generators_for_Monocular_Depth_Estimation_CVPR_2024_paper.html</span></li>
<li><span id="piccinelli2024unidepth">Piccinelli, L., Yang, Y.-H., &amp; al., E. (2024). UniDepth: Universal monocular metric depth estimation. <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</i>, 10106–10116.</span></li>
<li><span id="lowe2004distinctive">Lowe, D. G. (2004). Distinctive image features from scale-invariant keypoints. <i>International Journal of Computer Vision</i>, <i>60</i>, 91–110.</span></li>
<li><span id="sermanet2013overfeat">Sermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., &amp; LeCun, Y. (2013). Overfeat: Integrated recognition, localization and detection using convolutional networks. <i>ArXiv Preprint ArXiv:1312.6229</i>. https://arxiv.org/abs/1312.6229</span></li>
<li><span id="redmon2016you">Redmon, J., Divvala, S., Girshick, R., &amp; Farhadi, A. (2016). You only look once: Unified, real-time object detection. <i>Proceedings of the IEEE Conference on CVPR</i>, 779–788. https://arxiv.org/abs/1506.02640</span></li>
<li><span id="liu2016ssd">Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.-Y., &amp; Berg, A. C. (2016). Ssd: Single shot multibox detector. <i>Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part I 14</i>, 21–37. https://arxiv.org/abs/1512.02325</span></li>
<li><span id="girshick2014rich">Girshick, R., Donahue, J., Darrell, T., &amp; Malik, J. (2014). Rich feature hierarchies for accurate object detection and semantic segmentation. <i>Proceedings of the IEEE CVPR</i>, 580–587. https://arxiv.org/abs/1311.2524</span></li>
<li><span id="purkait2017spp">Purkait, P., Zhao, C., &amp; Zach, C. (2017). SPP-Net: Deep absolute pose regression with synthetic views. <i>ArXiv Preprint ArXiv:1712.03452</i>. https://arxiv.org/abs/1712.03452</span></li>
<li><span id="girshick2015fast">Girshick, R. (2015). Fast r-cnn. <i>Proceedings of the IEEE International Conference on Computer Vision</i>, 1440–1448. https://arxiv.org/abs/1504.08083</span></li>
<li><span id="ren2016faster">Ren, S., He, K., Girshick, R., &amp; Sun, J. (2016). Faster R-CNN: Towards real-time object detection with region proposal networks. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>, <i>39</i>(6), 1137–1149. https://arxiv.org/abs/1506.01497</span></li>
<li><span id="wu2020recent">Wu, X., Sahoo, D., &amp; Hoi, S. C. H. (2020). Recent advances in deep learning for object detection. <i>Neurocomputing</i>, <i>396</i>, 39–64. https://www.sciencedirect.com/science/article/pii/S0925231220301430</span></li>
<li><span id="pagire2025comprehensive">Pagire, V., Chavali, M., &amp; Kale, A. (2025). A comprehensive review of object detection with traditional and dl methods. <i>Signal Processing</i>, <i>237</i>, 110075. https://www.sciencedirect.com/science/article/pii/S0165168425001896</span></li>
<li><span id="zou2023object">Zou, Z., Chen, K., Shi, Z., Guo, Y., &amp; Ye, J. (2023). Object detection in 20 years: A survey. <i>Proceedings of the IEEE</i>, <i>111</i>(3), 257–276. https://ieeexplore.ieee.org/abstract/document/10028728</span></li>
<li><span id="sun2024evolution">Sun, Y., Sun, Z., &amp; Chen, W. (2024). The evolution of object detection methods. <i>Engineering Applications of Artificial Intelligence</i>, <i>133</i>, 108458. https://www.sciencedirect.com/science/article/pii/S095219762400616X</span></li>
<li><span id="chen20232d">Chen, W., Li, Y., Tian, Z., &amp; Zhang, F. (2023). 2D and 3D object detection algorithms from images: A Survey. <i>Array</i>, <i>19</i>, 100305. https://www.sciencedirect.com/science/article/pii/S2590005623000309</span></li>
<li><span id="carion2020end">Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., &amp; Zagoruyko, S. (2020). End-to-end object detection with transformers. <i>European Conference on Computer Vision</i>, 213–229. https://arxiv.org/abs/2005.12872</span></li>
<li><span id="zong2023detrs">Zong, Z., Song, G., &amp; Liu, Y. (2023). Detrs with collaborative hybrid assignments training. <i>Proceedings of the IEEE/CVF International Conference on Computer Vision</i>, 6748–6758. https://arxiv.org/abs/2211.12860</span></li>
<li><span id="zhu2020deformable">Zhu, X., Su, W., Lu, L., Li, B., Wang, X., &amp; Dai, J. (2020). Deformable detr: Deformable transformers for end-to-end object detection. <i>ArXiv Preprint ArXiv:2010.04159</i>. https://arxiv.org/abs/2010.04159</span></li>
<li><span id="oquab2023dinov2">Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V., Fernandez, P., Haziza, D., Massa, F., El-Nouby, A., &amp; others. (2023). Dinov2: Learning robust visual features without supervision. <i>ArXiv Preprint ArXiv:2304.07193</i>. https://arxiv.org/abs/2304.07193</span></li>
<li><span id="dai2016r">Dai, J., Li, Y., He, K., &amp; Sun, J. (2016). R-fcn: Object detection via region-based fully convolutional networks. <i>Advances in Neural Information Processing Systems</i>, <i>29</i>. https://arxiv.org/abs/1605.06409</span></li>
<li><span id="he2017mask">He, K., Gkioxari, G., Dollár, P., &amp; Girshick, R. (2017). Mask r-cnn. <i>Proceedings of the IEEE International Conference on Computer Vision</i>, 2961–2969. https://arxiv.org/abs/1703.06870l</span></li>
<li><span id="cai2018cascade">Cai, Z., &amp; Vasconcelos, N. (2018). Cascade r-cnn: Delving into high quality object detection. <i>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</i>, 6154–6162. https://arxiv.org/abs/1712.00726</span></li>
<li><span id="lin2017focal">Lin, T.-Y., Goyal, P., Girshick, R., He, K., &amp; Dollár, P. (2017). Focal loss for dense object detection. <i>Proceedings of the IEEE International Conference on Computer Vision</i>, 2980–2988. https://arxiv.org/abs/1708.02002</span></li>
<li><span id="law2018cornernet">Law, H., &amp; Deng, J. (2018). Cornernet: Detecting objects as paired keypoints. <i>Proceedings of the European Conference on Computer Vision (ECCV)</i>, 734–750. https://arxiv.org/abs/1808.01244</span></li>
<li><span id="liu2021swin">Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., &amp; Guo, B. (2021). Swin transformer: Hierarchical vision transformer using shifted windows. <i>Proceedings of the IEEE/CVF International Conference on Computer Vision</i>, 10012–10022. https://arxiv.org/abs/2103.14030</span></li>
<li><span id="tian2019fcos">Tian, Z., Shen, C., Chen, H., &amp; He, T. (2019). Fcos: Fully convolutional one-stage object detection. <i>Proceedings of the IEEE/CVF International Conference on Computer Vision</i>, 9627–9636. https://arxiv.org/abs/1904.01355</span></li>
<li><span id="duan2019centernet">Duan, K., Bai, S., Xie, L., Qi, H., Huang, Q., &amp; Tian, Q. (2019). Centernet: Keypoint triplets for object detection. <i>Proceedings of the IEEE/CVF International Conference on CVPR</i>, 6569–6578. https://arxiv.org/abs/1904.08189</span></li>
<li><span id="tan2020efficientdet">Tan, M., Pang, R., &amp; Le, Q. V. (2020). Efficientdet: Scalable and efficient object detection. <i>Proceedings of the IEEE/CVF Conference on CVPR</i>, 10781–10790. https://arxiv.org/abs/1911.09070</span></li>
<li><span id="wang2023internimage">Wang, W., Dai, J., Chen, Z., Huang, Z., Li, Z., Zhu, X., Hu, X., Lu, T., Lu, L., Li, H., &amp; others. (2023). Internimage: Exploring large-scale vision foundation models with deformable convolutions. <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</i>, 14408–14419. https://arxiv.org/abs/2211.05778</span></li>
<li><span id="li2022vitdet">Li, Y., Mao, H., Girshick, R., &amp; He, K. (2022). Exploring plain vision transformer backbones for object detection. <i>European Conference on Computer Vision</i>, 280–296. https://arxiv.org/abs/2203.16527</span></li>
<li><span id="tian2025yolov12">Tian, Y., Ye, Q., &amp; Doermann, D. (2025). Yolov12: Attention-centric real-time object detectors. <i>ArXiv Preprint ArXiv:2502.12524</i>. https://arxiv.org/abs/2502.12524</span></li>
<li><span id="zhao2024detrs">Zhao, Y., Lv, W., Xu, S., Wei, J., Wang, G., Dang, Q., Liu, Y., &amp; Chen, J. (2024). Detrs beat yolos on real-time object detection. <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</i>, 16965–16974. https://arxiv.org/abs/2304.08069</span></li>
<li><span id="xiang2015data">Xiang, Y., Choi, W., Lin, Y., &amp; Savarese, S. (2015). Data-driven 3d voxel patterns for object category recognition. <i>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</i>, 1903–1911. https://ieeexplore.ieee.org/document/7298800</span></li>
<li><span id="xiang2017subcategory">Xiang, Y., Choi, W., Lin, Y., &amp; Savarese, S. (2017). Subcategory-aware convolutional neural networks for object proposals and detection. <i>2017 IEEE Winter Conference on Applications of Computer Vision (WACV)</i>, 924–933. https://arxiv.org/abs/1604.04693</span></li>
<li><span id="chen2016monocular">Chen, X., Kundu, K., Zhang, Z., Ma, H., Fidler, S., &amp; Urtasun, R. (2016). Monocular 3d object detection for autonomous driving. <i>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</i>, 2147–2156. https://ieeexplore.ieee.org/document/7780605</span></li>
<li><span id="mousavian20173d">Mousavian, A., Anguelov, D., Flynn, J., &amp; Kosecka, J. (2017). 3d bounding box estimation using deep learning and geometry. <i>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</i>, 7074–7082. https://arxiv.org/abs/1612.00496</span></li>
<li><span id="chabot2017deep">Chabot, F., Chaouch, M., Rabarisoa, J., Teuliere, C., &amp; Chateau, T. (2017). Deep manta: A coarse-to-fine many-task network for joint 2d and 3d vehicle analysis from monocular image. <i>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</i>, 2040–2049. https://arxiv.org/abs/1703.07570</span></li>
<li><span id="kundu20183d">Kundu, A., Li, Y., &amp; Rehg, J. M. (2018). 3d-rcnn: Instance-level 3d object reconstruction via render-and-compare. <i>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</i>, 3559–3568. https://ieeexplore.ieee.org/document/8578473</span></li>
<li><span id="xu2018multi">Xu, B., &amp; Chen, Z. (2018). Multi-level fusion based 3d object detection from monocular images. <i>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</i>, 2345–2353. https://ieeexplore.ieee.org/document/8578347/</span></li>
<li><span id="qin2019monogrnet">Qin, Z., Wang, J., &amp; Lu, Y. (2019). Monogrnet: A geometric reasoning network for monocular 3d object localization. <i>Proceedings of the AAAI Conference on Artificial Intelligence</i>, <i>33</i>. https://arxiv.org/abs/1811.10247</span></li>
<li><span id="wang2023monoskd">Wang, S., &amp; Zheng, J. (2023). MonoSKD: General distillation framework for monocular 3D object detection via Spearman correlation coefficient. <i>ArXiv Preprint ArXiv:2310.11316</i>. https://arxiv.org/abs/2310.11316</span></li>
<li><span id="xu2023mononerd">Xu, J., Peng, L., Cheng, H., Li, H., Qian, W., Li, K., Wang, W., &amp; Cai, D. (2023). Mononerd: Nerf-like representations for monocular 3d object detection. <i>Proceedings of the IEEE/CVF International Conference on Computer Vision</i>, 6814–6824. https://arxiv.org/abs/2308.09421</span></li>
<li><span id="yan2024monocd">Yan, L., Yan, P., Xiong, S., Xiang, X., &amp; Tan, Y. (2024). Monocd: Monocular 3d object detection with complementary depths. <i>IEE</i>, 10248–10257. https://arxiv.org/abs/2404.03181</span></li>
<li><span id="chen20153d">Chen, X., Kundu, K., Zhu, Y., Berneshawi, A. G., Ma, H., Fidler, S., &amp; Urtasun, R. (2015). 3d object proposals for accurate object class detection. <i>Advances in Neural Information Processing Systems</i>, <i>28</i>. https://arxiv.org/abs/1608.07711</span></li>
<li><span id="wang2019pseudo">Wang, Y., Chao, W.-L., &amp; al., E. (2019). Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving. <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</i>, 8445–8453. https://arxiv.org/abs/1812.07179</span></li>
<li><span id="li2019stereo">Li, P., Chen, X., &amp; Shen, S. (2019). Stereo r-cnn based 3d object detection for autonomous driving. <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</i>. https://arxiv.org/abs/1902.09738</span></li>
<li><span id="chen2020dsgn">Chen, Y., Liu, S., Shen, X., &amp; Jia, J. (2020). Dsgn: Deep stereo geometry network for 3d object detection. <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</i>. https://arxiv.org/pdf/2001.03398</span></li>
<li><span id="froysa2018perception">Frøysa, T. D. (2018). <i>Perception for an Autonomous Racecar</i> [Master's thesis]. NTNU.</span></li>
<li><span id="qie2020cone">Qie, L., Gong, J., &amp; al., E. (2020). Cone detection and location for formula student driverless race. <i>2019 6th International Conference on Dependable Systems and Their Applications</i>, 440–444.</span></li>
<li><span id="gonzalez2020improved">Gonzalez, R. (2020). <i>Improved cone detection system with NN for a Formula Student car</i> [B.S. thesis]. Universitat Politècnica de Catalunya.</span></li>
<li><span id="dhall2018real">Dhall, A. (2018). Real-time 3D pose estimation with a monocular camera using deep learning and object priors on an autonomous racecar. <i>ArXiv Preprint ArXiv:1809.10548</i>.</span></li>
<li><span id="minorello2025stereo">Minorello, F. (2025). <i>A Stereo Vision SLAM Front-End for the Formula Student Driverless Competition</i> [Master's thesis]. University of Padova.</span></li>
<li><span id="ros">Quigley, M., Conley, K., &amp; al., E. (2009). ROS: an open-source Robot Operating System. <i>ICRA Workshop on Open Source Software</i>, <i>3</i>, 5. http://lars.mec.ua.pt/public/LAR%20Projects/BinPicking/2016_RodrigoSalgueiro/LIB/ROS/icraoss09-ROS.pdf</span></li>
<li><span id="wang2022detr3d">Wang, Y., Guizilini, V. C., Zhang, T., Wang, Y., Zhao, H., &amp; Solomon, J. (2022). Detr3d: 3d object detection from multi-view images via 3d-to-2d queries. <i>Conference on Robot Learning</i>, 180–191. https://arxiv.org/abs/2110.06922</span></li>
<li><span id="liu2022petr">Liu, Y., Wang, T., Zhang, X., &amp; Sun, J. (2022). Petr: Position embedding transformation for multi-view 3d object detection. <i>European Conference on Computer Vision</i>, 531–548. https://arxiv.org/abs/2203.05625</span></li>
<li><span id="li2024bevformer">Li, Z., Wang, W., &amp; al., E. (2024). Bevformer: learning bird’s-eye-view representation from lidar-camera via spatiotemporal transformers. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>. https://arxiv.org/abs/2203.17270</span></li>
<li><span id="liu2023sparsebev">Liu, H., Teng, Y., Lu, T., Wang, H., &amp; Wang, L. (2023). Sparsebev: High-performance sparse 3d object detection from multi-camera videos. <i>Proceedings of the IEEE/CVF International Conference on Computer Vision</i>, 18580–18590. https://arxiv.org/abs/2308.09244</span></li>
<li><span id="ji2025ropetr">Ji, H., Ni, T., Huang, X., Luo, T., Zhan, X., &amp; Chen, J. (2025). RoPETR: Improving Temporal Camera-Only 3D Detection by Integrating Enhanced Rotary Position Embedding. <i>ArXiv Preprint ArXiv:2504.12643</i>. https://arxiv.org/abs/2504.12643</span></li>
<li><span id="liu2023stereodistill">Liu, Z., Ye, X., Tan, X., Ding, E., &amp; Bai, X. (2023). Stereodistill: Pick the cream from lidar for distilling stereo-based 3d object detection. <i>Proceedings of the AAAI Conference on Artificial Intelligence</i>, <i>37</i>, 1790–1798. https://arxiv.org/pdf/2301.01615</span></li>
<li><span id="guo2021liga">Guo, X., Shi, S., Wang, X., &amp; Li, H. (2021). Liga-stereo: Learning lidar geometry aware representations for stereo-based 3d detector. <i>Proceedings of the IEEE/CVF International Conference on Computer Vision</i>, 3153–3163. https://arxiv.org/abs/2108.08258</span></li>
<li><span id="liu2021yolostereo3d">Liu, Y., Wang, L., &amp; Liu, M. (2021). Yolostereo3d: A step back to 2d for efficient stereo 3d detection. <i>2021 IEEE International Conference on Robotics and Automation (ICRA)</i>, 13018–13024. https://arxiv.org/abs/2103.09422</span></li>
<li><span id="brazil2019m3d">Brazil, G., &amp; Liu, X. (2019). M3d-rpn: Monocular 3d region proposal network for object detection. <i>Proceedings of the IEEE/CVF International Conference on Computer Vision</i>, 9287–9296. https://arxiv.org/abs/1907.06038</span></li>
<li><span id="liu2020smoke">Liu, Z., Wu, Z., &amp; Tóth, R. (2020). Smoke: Single-stage monocular 3d object detection via keypoint estimation. <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</i>, 996–997. https://arxiv.org/abs/2002.10111
</span></li>
<li><span id="limaye2020ss3d">Limaye, A., Mathew, M., &amp; al., E. (2020). SS3D: Single shot 3D object detector. <i>ArXiv Preprint ArXiv:2004.14674</i>. https://arxiv.org/abs/2004.14674</span></li>
<li><span id="zhang2021objects">Zhang, Y., Lu, J., &amp; Zhou, J. (2021). Objects are different: Flexible monocular 3d object detection. <i>Proceedings of the IEEE/CVF Conference on CVPR</i>, 3289–3298. https://arxiv.org/abs/2104.02323</span></li>
<li><span id="chong2022monodistill">Chong, Z., &amp; Ma, X. andE. al. (2022). Monodistill: Learning spatial features for monocular 3d object detection. <i>ArXiv Preprint ArXiv:2201.10830</i>. https://arxiv.org/pdf/2201.10830</span></li>
<li><span id="simonelli2019disentangling">Simonelli, A., Bulo, S. R., &amp; al., E. (2019). Disentangling monocular 3d object detection. <i>Proceedings of the IEEE/CVF International Conference on Computer Vision</i>, 1991–1999. https://arxiv.org/abs/1905.12365</span></li>
<li><span id="ma2019accurate">Ma, X., &amp; Wang, Z. andE. al. (2019). Accurate monocular 3d object detection via color-embedded 3d reconstruction for autonomous driving. <i>Proceedings of the IEEE/CVF International Conference on Computer Vision</i>, 6851–6860.</span></li>
<li><span id="choi2019multi">Choi, H. M., Kang, H., &amp; Hyun, Y. (2019). Multi-view reprojection architecture for orientation estimation. <i>2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)</i>, 2357–2366. https://ieeexplore.ieee.org/document/9022190</span></li>
<li><span id="zhou2018voxelnet">Zhou, Y., &amp; Tuzel, O. (2018). Voxelnet: End-to-end learning for point cloud based 3d object detection. <i>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</i>, 4490–4499.</span></li>
<li><span id="maturana2015votenet">Zhou, Y., &amp; Tuzel, O. (2018). VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection. <i>Proceedings of the IEEE Conference on CVPR</i>, 4490–4499. https://arxiv.org/abs/1611.08069</span></li>
<li><span id="simon2018complex">Simon, M., Milz, S., Amende, K., &amp; Gross, H.-M. (2018). Complex-yolo: Real-time 3d object detection on point clouds. <i>ArXiv Preprint ArXiv:1803.06199</i>. https://arxiv.org/abs/1803.06199</span></li>
<li><span id="beltran2018birdnet">Beltrán, J., Guindel, C., &amp; al., E. (2018). Birdnet: a 3d object detection framework from lidar information. <i>2018 21st International Conference on Intelligent Transportation Systems</i>, 3517–3523. https://arxiv.org/abs/1805.01195</span></li>
<li><span id="yang2018pixor">Yang, B., Luo, W., &amp; Urtasun, R. (2018). PIXOR: Real-time 3D Object Detection from Point Clouds. <i>Proceedings of the IEEE Conference on CVPR</i>, 7652–7660. https://arxiv.org/abs/1902.06326</span></li>
<li><span id="wang2015voting">Wang, D. Z., &amp; Posner, I. (2015). Voting for voting in online point cloud object detection. <i>Robotics: Science and Systems</i>, <i>1</i>, 10–15. https://www.roboticsproceedings.org/rss11/p35.pdf</span></li>
<li><span id="yan2018second">Yan, Y., Mao, Y., &amp; Li, B. (2018). SECOND: Sparsely Embedded Convolutional Detection. <i>Sensors</i>, <i>18</i>, 3337. https://pdfs.semanticscholar.org/5125/a16039cabc6320c908a4764f32596e018ad3.pdf</span></li>
<li><span id="lang2019pointpillars">Lang, A. H., Vora, S., Caesar, H., Zhou, L., Yang, J., &amp; Beijbom, O. (2019). PointPillars: Fast Encoders for Object Detection from Point Clouds. <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</i>, 12697–12705. https://arxiv.org/abs/1812.05784</span></li>
<li><span id="chen2020object">Chen, Q., Sun, L., Wang, Z., Jia, K., &amp; Yuille, A. (2020). Object as hotspots: An anchor-free 3d object detection approach via firing of hotspots. <i>Computer Vision–ECCV 2020: 16th European Conference</i>, 68–84. https://arxiv.org/abs/1912.12791</span></li>
<li><span id="deng2021voxel">Deng, J., Shi, S., Li, P., Zhou, W., Zhang, Y., &amp; Li, H. (2021). Voxel r-cnn: Towards high performance voxel-based 3d object detection. <i>Proceedings of the AAAI Conference on Artificial Intelligence</i>, <i>35</i>, 1201–1209. https://arxiv.org/abs/2012.15712</span></li>
<li><span id="mao2021voxel">Mao, J., Xue, Y., Niu, M., Bai, H., Feng, J., Liang, X., Xu, H., &amp; Xu, C. (2021). Voxel transformer for 3d object detection. <i>Proceedings of the IEEE/CVF International Conference on CVPR</i>, 3164–3173. https://arxiv.org/abs/2109.02497</span></li>
<li><span id="wu2023transformation">Wu, H., &amp; al., E. (2023). Transformation-equivariant 3d object detection for autonomous driving. <i>Proceedings of the AAAI Conference on Artificial Intelligence</i>, <i>37</i>, 2795–2802. https://arxiv.org/abs/2211.11962</span></li>
<li><span id="yang2019std">Yang, Z., Sun, Y., Liu, S., Shen, X., &amp; Jia, J. (2019). Std: Sparse-to-dense 3d object detector for point cloud. <i>Proceedings of the IEEE/CVF International Conference on CVPR</i>, 1951–1960. https://arxiv.org/abs/1907.10471</span></li>
<li><span id="yang20203dssd">Yang, Z., Sun, Y., Liu, S., &amp; Jia, J. (2020). 3dssd: Point-based 3d single stage object detector. <i>Proceedings of the IEEE/CVF Conference on CVPR</i>, 11040–11048. https://arxiv.org/abs/2002.10187</span></li>
<li><span id="pan20213d">Pan, X., Xia, Z., Song, S., Li, L. E., &amp; Huang, G. (2021). 3d object detection with pointformer. <i>Proceedings of the IEEE/CVF Conference on CVPR</i>, 7463–7472. https://arxiv.org/abs/2012.11409</span></li>
<li><span id="zhang2022not">Zhang, Y., Hu, Q., Xu, G., Ma, Y., Wan, J., &amp; Guo, Y. (2022). Not all points are equal: Learning highly efficient point-based detectors for 3d lidar point clouds. <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</i>, 18953–18962. https://arxiv.org/abs/2203.11139</span></li>
<li><span id="chen2019fast">Chen, Y., Liu, S., Shen, X., &amp; Jia, J. (2019). Fast point r-cnn. <i>Proceedings of the IEEE/CVF International Conference on Computer Vision</i>, 9775–9784. https://arxiv.org/abs/1908.02990</span></li>
<li><span id="shi2020pv">Shi, S., Guo, C., Jiang, L., Wang, Z., Shi, J., Wang, X., &amp; Li, H. (2020). Pv-rcnn: Point-voxel feature set abstraction for 3d object detection. <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</i>, 10529–10538. https://arxiv.org/abs/1912.13192</span></li>
<li><span id="hu2022point">Hu, J. S. K., Kuai, T., &amp; Waslander, S. L. (2022). Point density-aware voxels for lidar 3d object detection. <i>Proceedings of the IEEE/CVF Conference</i>, 8469–8478. https://arxiv.org/abs/2203.05662</span></li>
<li><span id="wang2019frustum">Wang, Z., &amp; Jia, K. (2019). Frustum convnet: Sliding frustums to aggregate local point-wise features for amodal 3d object detection. <i>2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</i>, 1742–1749. https://arxiv.org/abs/1903.01864</span></li>
<li><span id="qi2018frustum">Qi, C. R., Liu, W., Wu, C., Su, H., &amp; Guibas, L. J. (2018). Frustum pointnets for 3d object detection from rgb-d data. <i>Proceedings of the IEEE Conference on CVPR</i>, 918–927. https://arxiv.org/abs/1711.08488</span></li>
<li><span id="paigwar2021frustum">Paigwar, A., Sierra-Gonzalez, D., Erkent, Ö., &amp; Laugier, C. (2021). Frustum-pointpillars: A multi-stage approach for 3d object detection using rgb camera and lidar. <i>Proceedings of the IEEE/CVF International Conference on Computer Vision</i>, 2926–2933. https://arxiv.org/abs/1711.08488</span></li>
<li><span id="vora2020pointpainting">Vora, S., Lang, A. H., Helou, B., &amp; Beijbom, O. (2020). Pointpainting: Sequential fusion for 3d object detection. <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</i>, 4604–4612. https://arxiv.org/abs/1911.10150</span></li>
<li><span id="wu2023virtual">Wu, H., Wen, C., Shi, S., Li, X., &amp; Wang, C. (2023). Virtual sparse convolution for multimodal 3d object detection. <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</i>, 21653–21662. https://arxiv.org/abs/2303.02314</span></li>
<li><span id="pang2020clocs">Pang, S., Morris, D., &amp; Radha, H. (2020). CLOCs: Camera-LiDAR object candidates fusion for 3D object detection. <i>2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</i>, 10386–10393. https://arxiv.org/abs/2009.00784</span></li>
<li><span id="pang2022fast">Pang, S., Morris, D., &amp; Radha, H. (2022). Fast-CLOCs: Fast camera-LiDAR object candidates fusion for 3D object detection. <i>Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</i>, 187–196. https://openaccess.thecvf.com/content/WACV2022/papers/Pang_Fast-CLOCs_Fast_Camera-LiDAR_Object_Candidates_Fusion_for_3D_Object_Detection_WACV_2022_paper.pdf
</span></li>
<li><span id="chen2017multi">Chen, X., Ma, H., Wan, J., Li, B., &amp; Xia, T. (2017). Multi-view 3d object detection network for autonomous driving. <i>Proceedings of the IEEE Conference on CVPR</i>, 1907–1915. https://arxiv.org/abs/1611.07759</span></li>
<li><span id="ku2018joint">Ku, J., Mozifian, M., Lee, J., Harakeh, A., &amp; Waslander, S. L. (2018). Joint 3d proposal generation and object detection from view aggregation. <i>2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</i>, 1–8. https://arxiv.org/abs/1712.02294</span></li>
<li><span id="liang2018deep">Liang, M., Yang, B., Wang, S., &amp; Urtasun, R. (2018). Deep continuous fusion for multi-sensor 3d object detection. <i>Proceedings of the European Conference on Computer Vision (ECCV)</i>, 641–656. https://openaccess.thecvf.com/content_ECCV_2018/papers/Ming_Liang_Deep_Continuous_Fusion_ECCV_2018_paper.pdf</span></li>
<li><span id="liang2019multi">Liang, M., Yang, B., Chen, Y., Hu, R., &amp; Urtasun, R. (2019). Multi-task multi-sensor fusion for 3d object detection. <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</i>, 7345–7353. https://arxiv.org/abs/2012.12397</span></li>
<li><span id="huang2020epnet">Huang, T., Liu, Z., Chen, X., &amp; Bai, X. (2020). Epnet: Enhancing point features with image semantics for 3d object detection. <i>Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XV 16</i>, 35–52. https://arxiv.org/abs/2007.08856</span></li>
<li><span id="bai2022transfusion">Bai, X., Hu, Z., Zhu, X., Huang, Q., Chen, Y., Fu, H., &amp; Tai, C.-L. (2022). Transfusion: Robust lidar-camera fusion for 3d object detection with transformers. <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</i>, 1090–1099. https://arxiv.org/abs/2203.11496</span></li>
<li><span id="chen2023futr3d">Chen, X., Zhang, T., Wang, Y., Wang, Y., &amp; Zhao, H. (2023). Futr3d: A unified sensor fusion framework for 3d detection. <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</i>, 172–181. https://arxiv.org/abs/2203.10642</span></li>
<li><span id="candido2025uncrewed">Cândido, B., Santos, N. P., Moutinho, A., &amp; Zacchi, J.-V. (2025). Uncrewed Ground Vehicles in Military Operations: Lessons Learned from Experimental Exercises. <i>2025 IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC)</i>, 21–26. https://ieeexplore.ieee.org/document/10970121</span></li>
<li><span id="wang2021plumenet">Wang, Y., Yang, B., Hu, R., Liang, M., &amp; Urtasun, R. (2021). PLUMENet: Efficient 3D object detection from stereo images. <i>2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</i>, 3383–3390.</span></li>
<li><span id="shi2019pointrcnn">Shi, S., Wang, X., &amp; Li, H. (2019). Pointrcnn: 3d object proposal generation and detection from point cloud. <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</i>, 770–779. https://arxiv.org/abs/1812.04244</span></li>
<li><span id="shi2020points">Shi, S., Wang, Z., Shi, J., Wang, X., &amp; Li, H. (2020). From points to parts: 3d object detection from point cloud with part-aware and part-aggregation network. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>, <i>43</i>(8), 2647–2664. https://arxiv.org/abs/1907.03670</span></li>
<li><span id="he2020structure">He, C., Zeng, H., Huang, J., Hua, X.-S., &amp; Zhang, L. (2020). Structure aware single-stage 3d object detection from point cloud. <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</i>, 11873–11882.</span></li>
<li><span id="sindagi2019mvx">Sindagi, V. A., Zhou, Y., &amp; Tuzel, O. (2019). Mvx-net: Multimodal voxelnet for 3d object detection. <i>2019 International Conference on Robotics and Automation (ICRA)</i>, 7276–7282. https://arxiv.org/abs/1904.01649</span></li></ol>


  </main>

  <footer class="site-footer site-content">
    <p>&copy; 2025 3D Object Detection Hub</p>
  </footer>
</body>
</html>
